---
title: "Rereading Books"
author: "Amber Thomas"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
output: 
  puddingR::puddingTheme:
    toc: true
    code_folding: "hide"
    number_sections: "false"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, warning = FALSE)
```

## Introduction

Several months ago, I had a conversation with my coworkers at The Pudding about books, movies, and TV shows that I re-watch/re-read. I love diving back into forms of entertainment that I've experienced before. Sometimes I do it because I crave something familiar and the story seems warm and cozy. Sometimes I feel like I do this to prevent "food-envy", that is, the feeling I get when I try something new at a restaurant and wish that it was my tried and true favorite. Regardless of the reason, I found myself in the minority amongst my coworkers that actually likes to revisit media. It got me thinking about the books that other readers sink into time and time again. 


### Load Packages


```{r load_packages, message = FALSE, eval = TRUE}
# For general data cleaning and analysis
library(tidyverse)

# If your data includes dates that need to be wrangled
library(lubridate)

# For keeping your files in relative directories
library(here)

# For interactive/searchable tables in your report
library(DT)

# For Goodreads API
library(rgoodreads)

# For custom API access
library(httr)
library(xml2)
```

## Exploration
### My own data

To get a feel for how the Goodreads API works, I'll start by using it to export my own reader data and see if I can get the data I need just through the API.

```{r}
# set my gr key into global environment like this: Sys.setenv("GOODREADS_KEY"='x'))
```


```{r}
myData <- rgoodreads::user(16950883)
glimpse(myData)
```

Hmm, looks like that's not quite what I'm looking for. Maybe I'll have to spin up my own functions for this. 


```{r}
API_KEY <- Sys.getenv("GOODREADS_KEY")
URL <- "https://www.goodreads.com/review/list"
get_read_shelf <- function(userID) {
  shelf <- GET(URL, query = list(v = 2, key = API_KEY, id = userID, shelf = "read", per_page = 200))
  shelf_contents <- content(shelf, as = "parsed")
  return(shelf_contents)
}

myReadShelf <- get_read_shelf(1923002)

find_total <- function(shelf){
  kids <- xml_children(shelf)
  total <- xml_attr(kids[3], "total")
  
  tot <- shelf %>% 
    xml_attrs() 
  
  df <- tibble(total)
  
  return (df)
}

myTotal <- find_total(myReadShelf)

clean_shelf <- function(shelf){
  title <- shelf %>% 
    xml_find_all("//title") %>% 
    xml_text()
  
  author <- shelf %>% 
    xml_find_all("//author") %>% 
    xml_attrs() 
  
  read_count <- shelf %>% 
    xml_find_all("//read_count") %>% 
    xml_text()
  
  read_at <- shelf %>% 
    xml_find_all("//read_at") %>% 
    xml_text()
  
  rating <- shelf %>% 
    xml_find_all("//rating") %>% 
    xml_text()

  
  
  df <- tibble(title, author, read_count, read_at, rating)
  return(author)
}

myCleanShelf <- clean_shelf(myReadShelf)
```


Ok, so I have all the pieces, now to write a function that can cycle through all of the pages necessary. 


```{r}
parse_books <- function(contents, userID){
  title <- contents %>% 
    xml_find_all("//title") %>% 
    xml_text()
  
  author <- contents %>% 
    xml_find_all("//name") %>% 
    xml_text()
  
  read_count <- contents %>% 
    xml_find_all("//read_count") %>% 
    xml_text()
  
  read_at <- contents %>% 
    xml_find_all("//read_at") %>% 
    xml_text()
  
  rating <- contents %>% 
    xml_find_all("//rating") %>% 
    xml_text()
  
  df <- tibble(userID, title, author, read_count, read_at, rating)
  
  # define the location and filename for the new file
  fileName = here::here("assets", "data", "raw_data", "rereads.csv")

  # export the data to a csv
	write.table(df, file = fileName, row.names = FALSE, append = TRUE, sep = ",", col.names = !file.exists(fileName))
}
```

```{r}
get_first_page <- function(userID){
  URL <- "https://www.goodreads.com/review/list"
  # collects the data from the first page, including the total number of books on the user's "read" shelf
  shelf <- GET(URL, query = list(v = 2, key = API_KEY, id = userID, shelf = "read", per_page = 200, page = 1))
  status <- status_code(shelf)
  shelf_contents <- content(shelf, as = "parsed")
  
  # TODO: parse the shelf contents and export to file
  parse_books(shelf_contents, userID)
  
  # finds total number of books on "read" shelf and returns that number
  kids <- xml_children(shelf_contents)
  total <- xml_attr(kids[3], "total") %>% as.numeric()
  return(total)
}
```

```{r}
get_subsequent_pages <- function(userID, page_number){
  URL <- "https://www.goodreads.com/review/list"
  shelf <- GET(URL, query = list(v = 2, key = API_KEY, id = userID, shelf = "read", per_page = 200, page = page_number))
  shelf_contents <- content(shelf, as = "parsed")
  
  parse_books(shelf_contents, userID)
}
```


```{r}
add_ID <- function(userID){
  alreadyChecked <- as_tibble(userID)
  
  # define the location and filename for the new file
  fileName = here::here("assets", "data", "raw_data", "users.csv")

  # export the data to a csv
	write.table(alreadyChecked, file = fileName, row.names = FALSE, append = TRUE, sep = ",", col.names = !file.exists(fileName))
}
```

```{r}
find_rereads <- function(userID){
  API_KEY <- Sys.getenv("GOODREADS_KEY")
  URL <- "https://www.goodreads.com/review/list"

  total <- get_first_page(userID)
  
  if (class(total) == "numeric" & total > 200){
    totalPages <- ceiling(total / 200)
  
    # 200 results allowed per page
    breakpoints <- seq(2, totalPages, by = 1)
  
    walk2(userID, breakpoints, get_subsequent_pages)
  }
}

possibly_find_rereads <- purrr::possibly(find_rereads, otherwise = NA)
```

### Mara's Data
Let's start by looking at Mara Averick's read books (since she has quite a few)


```{r}
find_rereads(1923002)
```

```{r eval = TRUE}
mara <- read.csv(here::here("assets", "data", "raw_data", "rereads.csv"))
glimpse(mara)
```

Amazing, that seems to be working great! 

### Most Recent Reviewers

Now to get lots of user ids to parse through. The simplest way to do that may be to find the users that recently updated something and find all the books on their "read" shelf. Looks like this caps at 20 users, so I'll start there.

```{r}
find_recent_reviews <- function(){
  API_KEY <- Sys.getenv("GOODREADS_KEY")
  URL <- "https://www.goodreads.com/review/list"
  recent <- rgoodreads::recent_reviews() %>% 
    separate(col = user, c("userName", "userID"), ":")
  
  # load in the already checked ID's
  alreadyChecked <- read.csv(here::here("assets", "data", "raw_data", "users.csv"))
  
  # find only new IDs
  newIDs <- recent %>% 
    filter(!userID %in% alreadyChecked$value)
  
  # add new IDs to the list so they don't get double checked
  walk(newIDs$userID, add_ID)
  
  # find rereads for all users of recently reviewed things
  walk(newIDs$userID, possibly_find_rereads)
}

find_recent_reviews()
```

Let's try re-running this every 10 minutes for the next hour. 
```{r}
keep_finding_reviews <- function(i, .pb = NULL){
  # progress bar stuff
  if ((!is.null(.pb)) && inherits(.pb, "Progress") && (.pb$i < .pb$n)) .pb$tick()$print()
  
  # sleep for 10 minutes
  Sys.sleep(200)
  
  # look for recent reviews
  find_recent_reviews()
}

loops <- seq(1, 6)
pb <- progress_estimated(length(loops))
walk(loops, keep_finding_reviews, .pb = pb)
```


```{r eval = TRUE}
reads <- read.csv(here::here("assets", "data", "raw_data", "rereads.csv"))
```

Ok, so let's see how many books were reread.

```{r eval = TRUE}
rereads <- reads %>% 
  mutate(read_count = as.numeric(as.character(read_count))) %>% 
  filter(read_count > 1)

nrow(rereads)
```

```{r echo = FALSE, eval = TRUE}
readUsers <- reads %>% 
  mutate(userID = as.numeric(as.character(userID))) %>% 
  filter(!is.na(userID)) %>% 
  count(userID)

rereadUsers <- rereads %>% 
  mutate(userID = as.numeric(as.character(userID))) %>% 
  filter(!is.na(userID)) %>%
  count(userID)
```
Wow, ok, so out of the `r nrow(reads)` books that were read by these `r nrow(readUsers)` users, `r nrow(rereads)` (`r round(nrow(rereads)/nrow(reads) * 100)`%) were reread at least once. And, `r nrow(rereadUsers)` (`r round(nrow(rereadUsers)/nrow(readUsers) * 100)`%) of the readers randomly surveyed re-read books. 

Let's see what other questions we can start to answer.

### Most re-read book

Is everyone re-reading the same book?

```{r eval = TRUE}
uniqueBook <- rereads %>% 
  distinct(userID, title) %>% 
  count(title)

unique100 <- uniqueBook %>% 
  arrange(desc(n)) %>% 
  top_n(50)

unique100
```

Well, ok, looks like Harry Potter and other YA fantasy series (Hunger Games, Percy Jackson) are frequently re-read, along with some romance novels.

### How many times do users re-read?

Does a single user re-read the same story over and over? Or have they read several stories multiple times?

```{r eval = TRUE}
ggplot(rereads, aes(x = read_count)) + geom_histogram()
```

Ok, so most of the books were reread just twice. Let's look at the ones that users reread more than that.


```{r eval = TRUE}
readManyTimes <- rereads %>% 
  filter(read_count > 2) %>% 
  arrange(desc(read_count))

head(readManyTimes)
```

Wow, that one user *really* likes **The Hobbit**.

What's the average number of times different titles have been re-read?


```{r eval = TRUE}
avgReads <- rereads %>% 
  group_by(title, author) %>% 
  summarise(avg = mean(read_count), count = n()) %>% 
  arrange(desc(avg))

head(avgReads, 10)
```

Hmm, many of these favorites are re-read only by a single user. Maybe I should look at how many people have read each title at all.

```{r eval = TRUE}
overallRead <- reads %>% 
  mutate(read_count = as.numeric(as.character(read_count))) %>% 
  filter(!is.na(read_count)) %>% 
  filter(title %in% uniqueBook$title) %>% 
  group_by(title, author) %>% 
  summarise(avg = mean(read_count), count = n()) %>% 
  arrange(desc(avg))

head(overallRead)
```


### Date of re-reading

Lots of people are currently self-quarantined, has there been any increase in rereading old, comfortable favorites?

```{r eval = TRUE}
lastReRead <- rereads %>% 
  separate(read_at, into =c("weekday", "month", "day", "time", "zone", "year"), sep = " ", fill = "right") %>% 
  mutate(date = paste0(month, "-", day, "-", year)) %>% 
  mutate(date = lubridate::mdy(date)) %>% 
  group_by(year, month) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(year = as.numeric(as.character(year))) %>% 
  filter(!is.na(year)) %>% 
  filter(year >= 2017)

lrrGraph <- lastReRead %>% 
  mutate(year = as.factor(year), 
         month = factor(month, levels = month.abb))
```

```{r eval = TRUE}
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(lrrGraph, aes(month, fill = year)) + 
  geom_bar(aes(weight = count), position = "dodge") +
  scale_fill_manual(values = cbbPalette)
```

Wow, ok, so, there's definitely been a steady uptick in re-reads in early 2020. Though, the caveat to this is that I only have the date for the most *recent* re-read. Even so, it seems like people have been sinking their time into old favorites.

Let's see if the rate of reading overall has spiked during this time. 

```{r eval = TRUE}
lastRead <- reads %>% 
  separate(read_at, into =c("weekday", "month", "day", "time", "zone", "year"), sep = " ", fill = "right") %>% 
  mutate(date = paste0(month, "-", day, "-", year)) %>% 
  mutate(date = lubridate::mdy(date)) %>% 
  group_by(year, month) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(year = as.numeric(as.character(year))) %>% 
  filter(!is.na(year)) %>% 
  filter(year >= 2017)

lrGraph <- lastRead %>% 
  mutate(year = as.factor(year), 
         month = factor(month, levels = month.abb))
```

```{r eval = TRUE}
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(lrGraph, aes(month, fill = year)) + 
  geom_bar(aes(weight = count), position = "dodge") +
  scale_fill_manual(values = cbbPalette)
```

Ok, so we see a very similar pattern of increased reading happening at the start of 2020. What about the ratio of read to re-read books per month over this time span?

```{r eval = TRUE}
ratio <- lastRead %>% 
  rename(reads = count) %>% 
  left_join(lastReRead) %>% 
  rename(rereads = count) %>% 
  mutate(ratio = rereads/reads)

ratioGraph <- ratio %>% 
  mutate(year = as.factor(year), 
         month = factor(month, levels = month.abb)) %>% 
  arrange(year, month)
```

```{r eval = TRUE}
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(ratioGraph, aes(month, fill = year)) + 
  geom_bar(aes(weight = ratio), position = "dodge") +
  scale_fill_manual(values = cbbPalette)
```

Alright, so the ratio of re-reads to read books has been higher so far in March & April than it was at the same time in 2019, but not much higher than in 2018. 

```{r eval = TRUE}
ratioLine <- ratioGraph %>% 
  mutate(index = row_number())

ggplot(ratioLine, aes(x = index, y = ratio)) + geom_line()
```

